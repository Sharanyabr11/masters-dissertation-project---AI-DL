{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()  # Upload the kaggle.json file"
      ],
      "metadata": {
        "id": "iHt1_oQZ5KAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content\""
      ],
      "metadata": {
        "id": "1Lvl6f1p5hA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SnRkECf842K2"
      },
      "outputs": [],
      "source": [
        "!kaggle datasets download -d abdullahhasansajjad/lunghist700\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip lunghist700.zip -d /content/LungHist700"
      ],
      "metadata": {
        "id": "KQyt_pgI5zDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, RocCurveDisplay\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import models, layers\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# === 1. Data Splitting ===\n",
        "def split_dataset(original_data_dir, output_dir, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1, seed=42):\n",
        "    random.seed(seed)\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    train_dir = os.path.join(output_dir, \"train\")\n",
        "    val_dir = os.path.join(output_dir, \"val\")\n",
        "    test_dir = os.path.join(output_dir, \"test\")\n",
        "    os.makedirs(train_dir, exist_ok=True)\n",
        "    os.makedirs(val_dir, exist_ok=True)\n",
        "    os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "    for class_name in os.listdir(original_data_dir):\n",
        "        class_path = os.path.join(original_data_dir, class_name)\n",
        "        if not os.path.isdir(class_path):\n",
        "            continue\n",
        "        images = os.listdir(class_path)\n",
        "        random.shuffle(images)\n",
        "\n",
        "        n = len(images)\n",
        "        n_train = int(n * train_ratio)\n",
        "        n_val = int(n * val_ratio)\n",
        "        n_test = n - n_train - n_val\n",
        "\n",
        "        train_images = images[:n_train]\n",
        "        val_images = images[n_train:n_train+n_val]\n",
        "        test_images = images[n_train+n_val:]\n",
        "\n",
        "        for split_name, split_images in zip(\n",
        "            [train_dir, val_dir, test_dir],\n",
        "            [train_images, val_images, test_images]\n",
        "        ):\n",
        "            os.makedirs(os.path.join(split_name, class_name), exist_ok=True)\n",
        "            for img in split_images:\n",
        "                shutil.copy(os.path.join(class_path, img), os.path.join(split_name, class_name, img))\n",
        "\n",
        "    print(\"Dataset successfully split!\")\n",
        "\n",
        "# === Paths ===\n",
        "original_data_dir = \"/content/LungHist700/LungHist700_combined/data/images/\"\n",
        "output_dir = \"/content/output_split\"\n",
        "\n",
        "\n",
        "split_dataset(original_data_dir, output_dir)"
      ],
      "metadata": {
        "id": "XEZJSvwv56D4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === 2. Data Augmentation & Generators ===\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    horizontal_flip=True,\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    shear_range=0.1,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    os.path.join(output_dir, \"train\"),\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_generator = val_test_datagen.flow_from_directory(\n",
        "    os.path.join(output_dir, \"val\"),\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "test_generator = val_test_datagen.flow_from_directory(\n",
        "    os.path.join(output_dir, \"test\"),\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")"
      ],
      "metadata": {
        "id": "slovmVLS6IAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === 3. Compute Class Weights ===\n",
        "# Extract labels from train generator to compute weights\n",
        "y_train = []\n",
        "for _, labels in train_generator:\n",
        "    y_train.extend(np.argmax(labels, axis=1))\n",
        "    if len(y_train) >= train_generator.samples:\n",
        "        y_train = y_train[:train_generator.samples]\n",
        "        break\n",
        "\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(y_train),\n",
        "    y=y_train\n",
        ")\n",
        "class_weight_dict = {i: w for i, w in enumerate(class_weights)}\n",
        "print(\"Class weights:\", class_weight_dict)"
      ],
      "metadata": {
        "id": "ugIQjn5s7T4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# === 1. Load Pretrained Base ===\n",
        "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=IMG_SIZE + (3,))\n",
        "base_model.trainable = False  # Freeze all layers\n",
        "\n",
        "# === 2. Build Model on Top ===\n",
        "model = models.Sequential([\n",
        "    base_model,\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.Dense(train_generator.num_classes, activation='softmax')  # Output layer\n",
        "])\n",
        "\n",
        "# === 3. Compile the Model ===\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=1e-4),  # Set learning rate\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# === 4. Train the Model ===\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=60,  # You can increase this if needed\n",
        "    class_weight=class_weight_dict  # Apply class weights if there's imbalance\n",
        ")\n"
      ],
      "metadata": {
        "id": "eLQMkDlC74VE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === 6. Evaluate on Test Set ===\n",
        "test_loss, test_acc = model.evaluate(test_generator)\n",
        "print(f\"Test accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# Predictions and true labels\n",
        "y_pred_probs = model.predict(test_generator)\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "y_true = test_generator.classes\n",
        "\n",
        "# Classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=list(train_generator.class_indices.keys())))\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "print(\"Confusion Matrix:\\n\", cm)\n",
        "\n",
        "# Plot Confusion Matrix\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(7, 5))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\",\n",
        "            xticklabels=train_generator.class_indices.keys(),\n",
        "            yticklabels=train_generator.class_indices.keys())\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "pd2abINgEkq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === 7. Plot Training Graphs ===\n",
        "def plot_history(h):\n",
        "    plt.figure(figsize=(12,4))\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.plot(h.history['accuracy'], label='train_acc')\n",
        "    plt.plot(h.history['val_accuracy'], label='val_acc')\n",
        "    plt.legend()\n",
        "    plt.title('Accuracy')\n",
        "\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.plot(h.history['loss'], label='train_loss')\n",
        "    plt.plot(h.history['val_loss'], label='val_loss')\n",
        "    plt.legend()\n",
        "    plt.title('Loss')\n",
        "\n",
        "plot_history(history)\n",
        "plot_history(history_fine)"
      ],
      "metadata": {
        "id": "riQ8NIeHFEv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === 8. ROC-AUC (One-vs-Rest) for multi-class ===\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# Binarize labels for ROC-AUC calculation\n",
        "n_classes = train_generator.num_classes\n",
        "y_true_bin = label_binarize(y_true, classes=range(n_classes))\n",
        "\n",
        "# Compute ROC AUC for each class\n",
        "roc_auc = {}\n",
        "for i in range(n_classes):\n",
        "    roc_auc[i] = roc_auc_score(y_true_bin[:, i], y_pred_probs[:, i])\n",
        "\n",
        "print(\"ROC-AUC scores per class:\")\n",
        "for cls, score in zip(train_generator.class_indices.keys(), roc_auc.values()):\n",
        "    print(f\"{cls}: {score:.3f}\")\n",
        "\n",
        "# Plot ROC Curves\n",
        "plt.figure(figsize=(8, 6))\n",
        "for i, class_name in enumerate(train_generator.class_indices.keys()):\n",
        "    RocCurveDisplay.from_predictions(y_true_bin[:, i], y_pred_probs[:, i], name=class_name)\n",
        "plt.title(\"ROC Curves for each class\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YIKT26IMFIv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# y_true and y_pred are lists or numpy arrays of true labels and predicted labels\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "print(f\"Accuracy: {accuracy*100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "WK9zpNAc800q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}